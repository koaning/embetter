
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Scikit-Learn compatible embeddings">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../applications/">
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.2, mkdocs-material-9.1.21">
    
    
      
        <title>Finetuners - Embetter Docs</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.eebd395e.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ecc896b0.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetbrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Jetbrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#feeding-forward" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Embetter Docs" class="md-header__button md-logo" aria-label="Embetter Docs" data-md-component="logo">
      
  <img src="../images/icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Embetter Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Finetuners
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/koaning/embetter" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href=".." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  
    
  


  <li class="md-tabs__item">
    <a href="./" class="md-tabs__link md-tabs__link--active">
      Finetuners
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../applications/" class="md-tabs__link">
      Techniques
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../API/text/" class="md-tabs__link">
        API
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Embetter Docs" class="md-nav__button md-logo" aria-label="Embetter Docs" data-md-component="logo">
      
  <img src="../images/icon.png" alt="logo">

    </a>
    Embetter Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/koaning/embetter" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Finetuners
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Finetuners
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#feeding-forward" class="md-nav__link">
    Feeding Forward
  </a>
  
    <nav class="md-nav" aria-label="Feeding Forward">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#demo" class="md-nav__link">
    Demo
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation" class="md-nav__link">
    Evaluation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hyperparams" class="md-nav__link">
    Hyperparams
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extra-details" class="md-nav__link">
    Extra Details
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contrastive-methods" class="md-nav__link">
    Contrastive Methods
  </a>
  
    <nav class="md-nav" aria-label="Contrastive Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#demo_1" class="md-nav__link">
    Demo
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#more-learners" class="md-nav__link">
    More learners
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tuners-vs-learners" class="md-nav__link">
    Tuners vs. Learners
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance" class="md-nav__link">
    Performance
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../applications/" class="md-nav__link">
        Techniques
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/text/" class="md-nav__link">
        Text
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/vision/" class="md-nav__link">
        Vision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/multimodal/" class="md-nav__link">
        MultiModal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/external/" class="md-nav__link">
        External
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/finetune/" class="md-nav__link">
        Finetuners
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/model/" class="md-nav__link">
        Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Finetuners</h1>

<p>Embetter also supports tools to finetune the embedded space. This can be useful when you're trying to steer the embedding towards a task that you're interested in, which can make <a href="https://github.com/koaning/bulk/">bulk labelling</a> much easier. This guide will give an example of this. </p>
<h2 id="feeding-forward">Feeding Forward</h2>
<p>In general, this library is able to generate embeddings. </p>
<figure>
  <img src="../images/embed.png" width="60%" style="margin-left: auto;margin-right: auto;">
  <figcaption>Thing goes in. Vector goes out.</figcaption>
</figure>

<p>But the embeddings could eventually be the input of a neural network. So let's draw that.</p>
<figure>
  <img src="../images/feedforward.png" width="80%" style="margin-left: auto;margin-right: auto;">
  <figcaption>Thing goes in. Vector goes out. Then a feed forward network.</figcaption>
</figure>

<p>In this diagram, the network has an input layer of size <code>n</code>, which is provide by one of our embedding models. Next it has a hidden layer of size <code>k</code> and an output node. To make the drawing easier we've only draw a single node as output, but the argument will also work for any number of classes. </p>
<p>Let's now suppose that we train this model on a small set of labelled data. Then we'll have a gradient update that can update all the weights in this network.</p>
<figure>
  <img src="../images/gradient.png" width="80%" style="margin-left: auto;margin-right: auto;">
  <figcaption>The network has a gradient signal.</figcaption>
</figure>

<p>Here's the main trick: after we're done training, we don't output the predictions from the neural network! Instead, we might the hidden layer as the new embedding. </p>
<figure>
  <img src="../images/output.png" width="80%" style="margin-left: auto;margin-right: auto;">
  <figcaption>Notice how this layer "combines" the embedding and the label?</figcaption>
</figure>

<p>The thinking here is that this embedding will blend the information from the embedding, which hopefully is general, with the label that we're interested in, which is specific to our problem. Having such a blended embedding can be very useful for bulk labelling purposes, but if we pick our hyperparams right, we might even have an embedding that's a better fit for modelling.</p>
<p>There are many methods that we might use for finetuning and we've just explained the method used in the <code>FeedForwardTuner</code> component. </p>
<h3 id="demo">Demo</h3>
<p>Let's demonstrate this effect with a demo. We will use the imdb dataset, hosted on Huggingface, for our example here. This dataset contains movie reviews and the task is to predict if these are negative or positive reviews. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="n">imdb</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;imdb&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Next we'll prepare our data. We'll assume that we have 200 annotated examples. Let's call this our "train" set. We will encode this data with a sentence encoder. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">embetter.text</span> <span class="kn">import</span> <span class="n">SentenceEncoder</span>

<span class="c1"># Load up a sentence encoder.</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">SentenceEncoder</span><span class="p">()</span>

<span class="c1"># Assume we have 200 labels </span>
<span class="n">n_train</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Grab 200 examples and encode them</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">imdb</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()[:</span><span class="n">n_train</span><span class="p">])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">][:</span><span class="n">n_train</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</code></pre></div>
<p>Next we will also prepare a seperate set which we'll use to evaluate. This set is much larger, but we'll still pluck a subset to make the compute time shorter. </p>
<div class="highlight"><pre><span></span><code><span class="c1"># Let&#39;s grab 2000 examples for our &quot;test&quot; set </span>
<span class="n">n_test</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="c1"># Grab 2000 examples and encode them</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">imdb</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()[:</span><span class="n">n_test</span><span class="p">])</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">][:</span><span class="n">n_test</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</code></pre></div>
<p>Next we'll load our finetuner. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">embetter.finetune</span> <span class="kn">import</span> <span class="n">FeedForwardTuner</span> 

<span class="c1"># Create a network with some settings. You can totally change these. </span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">FeedForwardTuner</span><span class="p">(</span><span class="n">n_epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="c1"># Learn from our small training data</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<p>Given that we have a tuner trained, we can now apply it to our larger test set. </p>
<div class="highlight"><pre><span></span><code><span class="c1"># Note that it&#39;s all skearn compatible </span>
<span class="n">X_test_tfm</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div>
<h3 id="evaluation">Evaluation</h3>
<p>We just created <code>X_test_tfm</code>, which is a finetuned variant of <code>X_test</code>. To help
explain how the embedded space changed we'll make a PCA chart for both. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span> <span class="k">as</span> <span class="n">plt</span> 

<span class="n">X_orig</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">X_finetuned</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_test_tfm</span><span class="p">)</span>
</code></pre></div>
<p>Let's now show the difference.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># First chart </span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_orig</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="p">,</span> <span class="n">X_orig</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;PCA of original embedding space&quot;</span><span class="p">)</span>
</code></pre></div>
<figure>
  <img src="../images/x-orig.png" width="290%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>Notice how the two classes (positive/negative) are all mixed up when we look at the PCA plot of the embeddings. Let's now see what happens when we apply finetuning.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Second chart</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_finetuned</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="p">,</span> <span class="n">X_finetuned</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;PCA of fine-tuned embedding space&quot;</span><span class="p">)</span>
</code></pre></div>
<figure>
  <img src="../images/x-finetuned.png" width="290%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>The classes seem to separate much better! That's good news if you'd like to make selections for bulk labelling. It should be much easier to select the class that you're interested in, or to select from a region where there is plenty of doubt.</p>
<h3 id="hyperparams">Hyperparams</h3>
<p>It deserves mentioning that the effect on the PCA-space does depend a lot on the chosen hyperparameters of the <code>ForwardFinertuner</code>. </p>
<div class="highlight"><pre><span></span><code><span class="n">tuner</span> <span class="o">=</span> <span class="n">FeedForwardTuner</span><span class="p">(</span><span class="n">n_epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<p>If we decrease the hidden dimensions for example then we end up with a space that looks like this: </p>
<figure>
  <img src="../images/x-finetuned-again.png" width="290%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>You might want to play around with the settings, but it deserves to be said that you can also overfit on the few examples that you have in <code>X_train</code>.</p>
<h3 id="extra-details">Extra Details</h3>
<p>In scikit-learn terms, a tuner is a "transformer"-component. That means that it can be used in a pipeline too! </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span> 

<span class="c1"># Grab a few examples</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()[:</span><span class="mi">50</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()[:</span><span class="mi">50</span><span class="p">]</span>

<span class="c1"># Let&#39;s build a pipeline!</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">SentenceEncoder</span><span class="p">(),</span>
    <span class="n">FeedForwardTuner</span><span class="p">(</span><span class="n">n_epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">PCA</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1"># The fine-tuning component can use `y_train`.</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Apply all the trained steps! </span>
<span class="n">pipe</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
<p>Feel free to mix and match as you see fit. Also note that the tuner components in this library also support the <code>partial_fit</code> API incase you want to train on a stream of small batches.</p>
<h2 id="contrastive-methods">Contrastive Methods</h2>
<p>There is more than one way to finetune though. Instead of using a feed forward architecture, you can also opt
for a contrastive approach. In this approach two items are compared with eachother. The idea here is that similarity on pairs can also be the based on which to finetune towards a goal.</p>
<figure>
  <img src="../images/human-in-the-loop-1.png" width="90%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>This approach works by generating pairs of original embeddings. Some pairs will be positive, meaning they are embeddings of examples that belong to the same class. Others will be negatively sampled, meaning they don't share the same class. The embeddings get re-embedding with an extra embedding on top, which is determined by these pairs</p>
<figure>
  <img src="../images/human-in-the-loop-2.png" width="90%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>Note that in general this extra embedding layer is the same for both the items. On other words: these embeddings share the same weights. </p>
<figure>
  <img src="../images/human-in-the-loop-3.png" width="90%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>When you're done training such a system, you can re-use this trained embedding head to map the original embedding to a new space. The thinking is that this will lead to a better embedding.</p>
<figure>
  <img src="../images/human-in-the-loop-4.png" width="90%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>The benefit of this approach, compared to the feed forward one, is that you're flexible with how you generate pairs of examples. Are two examples part of the same label in a classification problem? Sure, that might be used. Doing something unsupervised and want two sentences from the same paragraph to be declared similar? Why not? Got image embeddings that you want to glue to text? You can really go nuts here, and this library will provide some tools to make it easy to bootstrap an approach using this technique.</p>
<h3 id="demo_1">Demo</h3>
<p>As a demonstration of this technique, we'll use data found in the <code>datasets</code> folder of this repository.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">srsly</span> 
<span class="kn">import</span> <span class="nn">itertools</span> <span class="k">as</span> <span class="nn">it</span> 
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">examples</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">it</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="n">srsly</span><span class="o">.</span><span class="n">read_jsonl</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;datasets&quot;</span><span class="p">)))</span>
</code></pre></div>
<p>This <code>examples</code> list contains examples that look like this:</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span><span class="err">&#39;</span><span class="kc">te</span><span class="err">x</span><span class="kc">t</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;Our</span><span class="w"> </span><span class="err">code</span><span class="w"> </span><span class="err">a</span><span class="kc">n</span><span class="err">d</span><span class="w"> </span><span class="err">da</span><span class="kc">taset</span><span class="w"> </span><span class="err">is</span><span class="w"> </span><span class="err">available</span><span class="w"> </span><span class="err">here.&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;ca</span><span class="kc">ts</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="err">&#39;</span><span class="kc">ne</span><span class="err">w</span><span class="mi">-</span><span class="err">da</span><span class="kc">taset</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;dev</span><span class="mi">-</span><span class="err">research&#39;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">}}</span>
</code></pre></div>
<p>The interesting thing in this dataset is that there are nested labels. For some examples we'll have all labels, but for others we may only have a subset.</p>
<div class="highlight"><pre><span></span><code><span class="n">labels</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">cat</span> <span class="ow">in</span> <span class="n">ex</span><span class="p">[</span><span class="s1">&#39;cats&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">cat</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">:</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">union</span><span class="p">([</span><span class="n">cat</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">labels</span> <span class="o">==</span> <span class="p">{</span><span class="s1">&#39;data-quality&#39;</span><span class="p">,</span> <span class="s1">&#39;dev-research&#39;</span><span class="p">,</span> <span class="s1">&#39;new-dataset&#39;</span><span class="p">}</span>
</code></pre></div>
<p>But from this we can generate pairs of examples that can be declared similar/dissimilar. </p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">random</span> 

<span class="k">def</span> <span class="nf">sample_generator</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">n_neg</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="s2">&quot;new-dataset&quot;</span><span class="p">:</span>
            <span class="n">pos_examples</span> <span class="o">=</span> <span class="p">[</span><span class="n">ex</span> <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">examples</span> <span class="k">if</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">ex</span><span class="p">[</span><span class="s1">&#39;cats&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">ex</span><span class="p">[</span><span class="s1">&#39;cats&#39;</span><span class="p">][</span><span class="n">label</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">neg_examples</span> <span class="o">=</span> <span class="p">[</span><span class="n">ex</span> <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">examples</span> <span class="k">if</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">ex</span><span class="p">[</span><span class="s1">&#39;cats&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">ex</span><span class="p">[</span><span class="s1">&#39;cats&#39;</span><span class="p">][</span><span class="n">label</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">pos_examples</span><span class="p">:</span>
                <span class="n">sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pos_examples</span><span class="p">)</span>
                <span class="k">yield</span> <span class="p">(</span><span class="n">ex</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="mf">1.0</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_neg</span><span class="p">):</span>
                    <span class="n">sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">neg_examples</span><span class="p">)</span>
                    <span class="k">yield</span> <span class="p">(</span><span class="n">ex</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">)</span>

<span class="n">learn_examples</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sample_generator</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">n_neg</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
<span class="n">texts1</span><span class="p">,</span> <span class="n">texts2</span><span class="p">,</span> <span class="n">similar</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">learn_examples</span><span class="p">)</span>
</code></pre></div>
<p>Here's what the <code>texts1</code>, <code>text2</code> and <code>similar</code> lists might include as an example.</p>
<table>
<thead>
<tr>
<th>Sentence A</th>
<th>Sentence B</th>
<th>Similar</th>
</tr>
</thead>
<tbody>
<tr>
<td>Our code and dataset is available here.</td>
<td>We release the resulting corpus and our analysis pipeline for future research.</td>
<td>1</td>
</tr>
<tr>
<td>Our code and dataset is available here.</td>
<td>In this paper, we introduce the predicted intent labels to calibrate answer labels in a self-training paradigm.</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>It's these kinds of pairs that we can try to learn from. So let's do this with a <code>ContrastiveLearner</code> by finetuning the embeddings provided to us from a <code>SentenceEncoder</code>. To do that, we'll first need to generate the data in a format that it can used. </p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">from</span> <span class="nn">embetter.text</span> <span class="kn">import</span> <span class="n">SentenceEncoder</span>
<span class="kn">from</span> <span class="nn">embetter.finetune</span> <span class="kn">import</span> <span class="n">ContrastiveLearner</span>

<span class="c1"># Generate numeric representations for the pairs</span>
<span class="n">sentence_encoder</span> <span class="o">=</span> <span class="n">SentenceEncoder</span><span class="p">(</span><span class="s1">&#39;all-MiniLM-L6-v2&#39;</span><span class="p">)</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">sentence_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">texts1</span><span class="p">),</span> <span class="n">sentence_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">texts2</span><span class="p">)</span>

<span class="c1"># This is a good habbit, numpy arrays are nicer to work with than tuples here</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">similar</span><span class="p">)</span>
</code></pre></div>
<p>With the data ready, we can train. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">embetter.finetune</span> <span class="kn">import</span> <span class="n">ContrastiveLearner</span>

<span class="n">learner</span> <span class="o">=</span> <span class="n">ContrastiveLearner</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.002</span><span class="p">,</span> <span class="n">shape_out</span><span class="o">=</span><span class="mi">384</span><span class="p">)</span>
<span class="n">learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>
<p>Note that <code>learner</code> types of finetuners accept two data inputs in <code>.fit(X1, X2, y)</code>-method. This is not what the scikit-learn API would allow in a pipeline, but it is a format that allows you to be flexible. </p>
<p>In this case the fine-tuning will be done quickly and we can generate new embeddings.</p>
<div class="highlight"><pre><span></span><code><span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">ex</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">examples</span> <span class="k">if</span> <span class="s1">&#39;new-dataset&#39;</span> <span class="ow">in</span> <span class="n">ex</span><span class="p">[</span><span class="s1">&#39;cats&#39;</span><span class="p">]]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ex</span><span class="p">[</span><span class="s1">&#39;cats&#39;</span><span class="p">][</span><span class="s1">&#39;new-dataset&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">examples</span> <span class="k">if</span> <span class="s1">&#39;new-dataset&#39;</span> <span class="ow">in</span> <span class="n">ex</span><span class="p">[</span><span class="s1">&#39;cats&#39;</span><span class="p">]])</span>

<span class="n">X_texts</span> <span class="o">=</span> <span class="n">sentence_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="n">X_texts_tfm</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_texts</span><span class="p">)</span>
</code></pre></div>
<p>For fun, we can also see if these new embeddings give us more predictive power. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="k">def</span> <span class="nf">calc_performance</span><span class="p">(</span><span class="n">X_in</span><span class="p">,</span> <span class="n">y_in</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="n">mod</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="s2">&quot;balanced&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_in</span><span class="p">,</span> <span class="n">y_in</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_in</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_in</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> got </span><span class="si">{</span><span class="n">acc</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">calc_performance</span><span class="p">(</span><span class="n">X_texts</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="s2">&quot;original embeddings&quot;</span><span class="p">)</span>
<span class="n">calc_performance</span><span class="p">(</span><span class="n">X_texts_tfm</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="s2">&quot;finetuned embeddings&quot;</span><span class="p">)</span>

<span class="c1"># original embeddings got acc=0.8624434389140272</span>
<span class="c1"># finetuned embeddings got acc=0.9180995475113122</span>
</code></pre></div>
<p>This isn't a proper benchmark, we're measuring the train set after all, but it does comfirm that the embeddings differ. If you're finetuning your own embeddings you should always think hard about how you'd like to evaluate this. </p>
<h3 id="more-learners">More learners</h3>
<p>This library also provides a learning that directly integrates with <code>sentence-transformers</code>. Training these is typically slower, because it involves finetuning an entire BERT pipeline but may provide solid results. One downside of this approach is that you'll have a learner than cannot accept general arrays. It must provide inputs that sentence-transformers can deal with, which it typically text.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">embetter.finetune</span> <span class="kn">import</span> <span class="n">SbertLearner</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Load in a sentence transformer manually</span>
<span class="n">sent_tfm</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;all-MiniLM-L6-v2&#39;</span><span class="p">)</span>

<span class="c1"># Pass it to the SbertLearner and train</span>
<span class="n">sbert_learn</span> <span class="o">=</span> <span class="n">SbertLearner</span><span class="p">(</span><span class="n">sent_tfm</span><span class="o">=</span><span class="n">sent_tfm</span><span class="p">)</span>
<span class="n">sbert_learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">texts1</span><span class="p">,</span> <span class="n">texts2</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="c1"># Once training is done, it can be used to encode embeddings</span>
<span class="c1"># Note that we input `texts`, not `X_texts`!</span>
<span class="n">X_texts_sbert</span> <span class="o">=</span> <span class="n">sbert_learn</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>

<span class="c1"># You can now save the new model which is stored in the original variable</span>
<span class="c1"># the `SbertLearner` object directly operates on it</span>
<span class="n">sent_tfm</span><span class="o">.</span><span class="n">to_disk</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</code></pre></div>
<h3 id="tuners-vs-learners"><code>Tuner</code>s vs. <code>Learner</code>s</h3>
<p>One downside of the <code>learner</code> objects is that they cannot be used in a scikit-learn pipeline during the <code>.fit()</code>-step because they have an incompatible API. To mitigate these, this library sometimes offers a "<code>Tuner</code>"-variant which has a "<code>Learner</code>"-variant of a method internally. Under the hood, a "tuner" will use a "learner" to make sure the finetuning works, but it won't be as flexible when it comes to training. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">embetter.finetune</span> <span class="kn">import</span> <span class="n">ContrastiveTuner</span>
<span class="kn">from</span> <span class="nn">embetter.text</span> <span class="kn">import</span> <span class="n">SentenceEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="c1"># Notice that we&#39;re using `tuner` here, not `learner`!</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">SentenceEncoder</span><span class="p">(),</span> <span class="n">Contrastivetuner</span><span class="p">())</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
<h3 id="performance">Performance</h3>
<p>This library favors ease of use over optimal performance, but it's certainly possible that the performance can be improved. If you have a clever suggestion, feel free to discuss it by opening <a href="https://github.com/koaning/embetter/issues">an issue</a>.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["toc.integrate", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.path", "navigation.indexes", "toc.follow", "content.code.copy", "content.code.select", "content.code.annotate"], "search": "../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.220ee61c.min.js"></script>
      
    
  </body>
</html>