
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Scikit-Learn compatible embeddings">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../applications/">
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.2, mkdocs-material-9.1.21">
    
    
      
        <title>Finetuners - Embetter Docs</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.eebd395e.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ecc896b0.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetbrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Jetbrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#how-it-works" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Embetter Docs" class="md-header__button md-logo" aria-label="Embetter Docs" data-md-component="logo">
      
  <img src="../images/icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Embetter Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Finetuners
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/koaning/embetter" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href=".." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  
    
  


  <li class="md-tabs__item">
    <a href="./" class="md-tabs__link md-tabs__link--active">
      Finetuners
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../applications/" class="md-tabs__link">
      Techniques
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../API/text/" class="md-tabs__link">
        API
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Embetter Docs" class="md-nav__button md-logo" aria-label="Embetter Docs" data-md-component="logo">
      
  <img src="../images/icon.png" alt="logo">

    </a>
    Embetter Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/koaning/embetter" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Finetuners
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Finetuners
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#how-it-works" class="md-nav__link">
    How it works
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#demo" class="md-nav__link">
    Demo
  </a>
  
    <nav class="md-nav" aria-label="Demo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluation" class="md-nav__link">
    Evaluation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hyperparams" class="md-nav__link">
    Hyperparams
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extra-details" class="md-nav__link">
    Extra Details
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contrastive-finetuners" class="md-nav__link">
    Contrastive Finetuners
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../applications/" class="md-nav__link">
        Techniques
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/text/" class="md-nav__link">
        Text
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/vision/" class="md-nav__link">
        Vision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/multimodal/" class="md-nav__link">
        MultiModal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/external/" class="md-nav__link">
        External
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/finetune/" class="md-nav__link">
        Finetuners
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/model/" class="md-nav__link">
        Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Finetuners</h1>

<p>Embetter also supports tools to finetune the embedded space. This can be useful when you're trying to steer the embedding towards a task that you're interested in, which can make <a href="https://github.com/koaning/bulk/">bulk labelling</a> much easier. This guide will give an example of this. </p>
<h2 id="how-it-works">How it works</h2>
<p>In general, this library is able to generate embeddings. </p>
<figure>
  <img src="../images/embed.png" width="60%" style="margin-left: auto;margin-right: auto;">
  <figcaption>Thing goes in. Vector goes out.</figcaption>
</figure>

<p>But the embeddings could eventually be the input of a neural network. So let's draw that.</p>
<figure>
  <img src="../images/feedforward.png" width="80%" style="margin-left: auto;margin-right: auto;">
  <figcaption>Thing goes in. Vector goes out. Then a feed forward network.</figcaption>
</figure>

<p>In this diagram, the network has an input layer of size <code>n</code>, which is provide by one of our embedding models. Next it has a hidden layer of size <code>k</code> and an output node. To make the drawing easier we've only draw a single node as output, but the argument will also work for any number of classes. </p>
<p>Let's now suppose that we train this model on a small set of labelled data. Then we'll have a gradient update that can update all the weights in this network.</p>
<figure>
  <img src="../images/gradient.png" width="80%" style="margin-left: auto;margin-right: auto;">
  <figcaption>The network has a gradient signal.</figcaption>
</figure>

<p>Here's the main trick: after we're done training, we don't output the predictions from the neural network! Instead, we might the hidden layer as the new embedding. </p>
<figure>
  <img src="../images/output.png" width="80%" style="margin-left: auto;margin-right: auto;">
  <figcaption>Notice how this layer "combines" the embedding and the label?</figcaption>
</figure>

<p>The thinking here is that this embedding will blend the information from the embedding, which hopefully is general, with the label that we're interested in, which is specific to our problem. Having such a blended embedding can be very useful for bulk labelling purposes, but if we pick our hyperparams right, we might even have an embedding that's a better fit for modelling.</p>
<p>There are many methods that we might use for finetuning and we've just explained the method used in the <code>FeedForwardFinetuner</code> component. </p>
<h2 id="demo">Demo</h2>
<p>Let's demonstrate this effect with a demo. We will use the imdb dataset, hosted on Huggingface, for our example here. This dataset contains movie reviews and the task is to predict if these are negative or positive reviews. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="n">imdb</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;imdb&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Next we'll prepare our data. We'll assume that we have 200 annotated examples. Let's call this our "train" set. We will encode this data with a sentence encoder. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">embetter.text</span> <span class="kn">import</span> <span class="n">SentenceEncoder</span>

<span class="c1"># Load up a sentence encoder.</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">SentenceEncoder</span><span class="p">()</span>

<span class="c1"># Assume we have 200 labels </span>
<span class="n">n_train</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Grab 200 examples and encode them</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">imdb</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()[:</span><span class="n">n_train</span><span class="p">])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">][:</span><span class="n">n_train</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</code></pre></div>
<p>Next we will also prepare a seperate set which we'll use to evaluate. This set is much larger, but we'll still pluck a subset to make the compute time shorter. </p>
<div class="highlight"><pre><span></span><code><span class="c1"># Let&#39;s grab 2000 examples for our &quot;test&quot; set </span>
<span class="n">n_test</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="c1"># Grab 2000 examples and encode them</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">imdb</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()[:</span><span class="n">n_test</span><span class="p">])</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">][:</span><span class="n">n_test</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</code></pre></div>
<p>Next we'll load our finetuner. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">embetter.finetune</span> <span class="kn">import</span> <span class="n">ForwardFinetuner</span> 

<span class="c1"># Create a network with some settings. You can totally change these. </span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">ForwardFinetuner</span><span class="p">(</span><span class="n">n_epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="c1"># Learn from our small training data</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<p>Given that we have a tuner trained, we can now apply it to our larger test set. </p>
<div class="highlight"><pre><span></span><code><span class="c1"># Note that it&#39;s all skearn compatible </span>
<span class="n">X_test_tfm</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div>
<h3 id="evaluation">Evaluation</h3>
<p>We just created <code>X_test_tfm</code>, which is a finetuned variant of <code>X_test</code>. To help
explain how the embedded space changed we'll make a PCA chart for both. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span> <span class="k">as</span> <span class="n">plt</span> 

<span class="n">X_orig</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">X_finetuned</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_test_tfm</span><span class="p">)</span>
</code></pre></div>
<p>Let's now show the difference.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># First chart </span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_orig</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="p">,</span> <span class="n">X_orig</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;PCA of original embedding space&quot;</span><span class="p">)</span>
</code></pre></div>
<figure>
  <img src="../images/x-orig.png" width="290%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>Notice how the two classes (positive/negative) are all mixed up when we look at the PCA plot of the embeddings. Let's now see what happens when we apply finetuning.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Second chart</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_finetuned</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="p">,</span> <span class="n">X_finetuned</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;PCA of fine-tuned embedding space&quot;</span><span class="p">)</span>
</code></pre></div>
<figure>
  <img src="../images/x-finetuned.png" width="290%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>The classes seem to separate much better! That's good news if you'd like to make selections for bulk labelling. It should be much easier to select the class that you're interested in, or to select from a region where there is plenty of doubt.</p>
<h3 id="hyperparams">Hyperparams</h3>
<p>It deserves mentioning that the effect on the PCA-space does depend a lot on the chosen hyperparameters of the <code>ForwardFinertuner</code>. </p>
<div class="highlight"><pre><span></span><code><span class="n">tuner</span> <span class="o">=</span> <span class="n">ForwardFinetuner</span><span class="p">(</span><span class="n">n_epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<p>If we decrease the hidden dimensions for example then we end up with a space that looks like this: </p>
<figure>
  <img src="../images/x-finetuned-again.png" width="290%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>You might want to play around with the settings, but it deserves to be said that you can also overfit on the few examples that you have in <code>X_train</code>.</p>
<h3 id="extra-details">Extra Details</h3>
<p>In scikit-learn terms, a fine-tuner is a "transformer"-component. That means that it can be used in a pipeline too! </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span> 

<span class="c1"># Grab a few examples</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()[:</span><span class="mi">50</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()[:</span><span class="mi">50</span><span class="p">]</span>

<span class="c1"># Let&#39;s build a pipeline!</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">SentenceEncoder</span><span class="p">(),</span>
    <span class="n">ForwardFinetuner</span><span class="p">(</span><span class="n">n_epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">PCA</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1"># The fine-tuning component can use `y_train`.</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Apply all the trained steps! </span>
<span class="n">pipe</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
<p>Feel free to mix and match as you see fit. Also note that the finetuning components in this library also support the <code>partial_fit</code> API incase you want to train on a stream of small batches.</p>
<h2 id="contrastive-finetuners">Contrastive Finetuners</h2>
<p>There is more than one way to finetune though. Instead of using a feed forward architecture, you can also opt
for a contrastive approach. </p>
<figure>
  <img src="../images/contrastive.png" width="90%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>This approach works by generating pairs of original embeddings. Some pairs will be positive, meaning they are embeddings of examples that belong to the same class. Others will be negatively sampled, meaning they don't share the same class. The embeddings get re-embedding with a linear layer such that they're able to adapt depending on the label.</p>
<figure>
  <img src="../images/contrastive-same-weights.png" width="90%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>The embedding layers in the contrastive network share the same weights and they both get updated during the gradient update. Then, at interference, 
we end up with new embeddings because we can re-use the learned contrastive embedding layer. </p>
<figure>
  <img src="../images/contrastive-re-use.png" width="90%" style="margin-left: auto;margin-right: auto;">
</figure>

<p>You can experiment with this approach by importing the </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">embetter.finetune</span> <span class="kn">import</span> <span class="n">ContrastiveFinetuner</span>

<span class="n">n_train</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">texts</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()[:</span><span class="n">n_train</span><span class="p">]</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">][:</span><span class="n">n_train</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># These are original embeddings</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">SentenceEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

<span class="c1"># These are fine-tuned embeddings</span>
<span class="n">X_tfm</span> <span class="o">=</span> <span class="n">ContrastiveFinetuner</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</code></pre></div>
<p>We're still experimenting with both approaches to finetuning to try and understand when each approach is better.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["toc.integrate", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.path", "navigation.indexes", "toc.follow", "content.code.copy", "content.code.select", "content.code.annotate"], "search": "../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.220ee61c.min.js"></script>
      
    
  </body>
</html>