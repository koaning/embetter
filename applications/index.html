
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Scikit-Learn compatible embeddings">
      
      
      
      
        <link rel="prev" href="../finetuners/">
      
      
        <link rel="next" href="../API/text/">
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.2, mkdocs-material-9.1.21">
    
    
      
        <title>Techniques - Embetter Docs</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.eebd395e.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ecc896b0.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetbrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Jetbrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#cache" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Embetter Docs" class="md-header__button md-logo" aria-label="Embetter Docs" data-md-component="logo">
      
  <img src="../images/icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Embetter Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Techniques
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/koaning/embetter" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href=".." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../finetuners/" class="md-tabs__link">
      Finetuners
    </a>
  </li>

      
        
  
  
    
  


  <li class="md-tabs__item">
    <a href="./" class="md-tabs__link md-tabs__link--active">
      Techniques
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../API/text/" class="md-tabs__link">
        API
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Embetter Docs" class="md-nav__button md-logo" aria-label="Embetter Docs" data-md-component="logo">
      
  <img src="../images/icon.png" alt="logo">

    </a>
    Embetter Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/koaning/embetter" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../finetuners/" class="md-nav__link">
        Finetuners
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Techniques
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Techniques
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#cache" class="md-nav__link">
    Cache
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lite-embeddings" class="md-nav__link">
    Lite Embeddings
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#difference-models" class="md-nav__link">
    Difference Models
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#available-sentenceencoders" class="md-nav__link">
    Available SentenceEncoders
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#speedup-with-modal" class="md-nav__link">
    Speedup with Modal
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/text/" class="md-nav__link">
        Text
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/vision/" class="md-nav__link">
        Vision
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/multimodal/" class="md-nav__link">
        MultiModal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/external/" class="md-nav__link">
        External
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/finetune/" class="md-nav__link">
        Finetuners
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../API/model/" class="md-nav__link">
        Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Techniques</h1>

<p>This document contains some tricks, hints and demos of applications that you might want to consider
in combination with this library. </p>
<h2 id="cache">Cache</h2>
<p>Calculating embeddings can be expensive, even costly when you're using external providers. 
This is why this library offers an integration with <a href="https://grantjenks.com/docs/diskcache/">diskcache</a>. 
That way, you can infer the embeddings once and store them to disk for later.</p>
<p>Here's an example of how you might run that. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">embetter.text</span> <span class="kn">import</span> <span class="n">SentenceEncoder</span>
<span class="kn">from</span> <span class="nn">embetter.utils</span> <span class="kn">import</span> <span class="n">cached</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">cached</span><span class="p">(</span><span class="s2">&quot;sentence-enc&quot;</span><span class="p">,</span> <span class="n">SentenceEncoder</span><span class="p">(</span><span class="s1">&#39;all-MiniLM-L6-v2&#39;</span><span class="p">))</span>

<span class="n">examples</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;this is a pretty long text, which is more expensive </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10_000</span><span class="p">)]</span>

<span class="c1"># This might be a bit slow ~17.2s on our machine</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>

<span class="c1"># This should be quicker ~4.71s on our machine</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>
</code></pre></div>
<p>Note that you're also able to fetch the precalculated embeddings directly via: </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">diskcache</span> <span class="kn">import</span> <span class="n">Cache</span>

<span class="c1"># Make sure that you use the same name as in `cached`</span>
<span class="n">cache</span> <span class="o">=</span> <span class="n">Cache</span><span class="p">(</span><span class="s2">&quot;sentence-enc&quot;</span><span class="p">)</span>
<span class="c1"># Use a string as a key, if it&#39;s precalculated you&#39;ll get an array back.</span>
<span class="n">cache</span><span class="p">[</span><span class="s2">&quot;this is a pretty long text, which is more expensive 0&quot;</span><span class="p">]</span>
</code></pre></div>
<p>Be mindful of what does in to the encoder that you choose. It's preferable to give it
text as opposed to numpy arrays. Also note that the first time that you'll run this
it will take more time due to the overhead of writing into the cache.</p>
<h2 id="lite-embeddings">Lite Embeddings</h2>
<p>There are a lot of options out there for pretrained text embeddings but there are also a few noteworthy lightweight techniques that allow you to train your own from scratch. One such technique is to use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TfidfVectorizer</a> 
from scikit-learn followed by <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html">TruncatedSVD</a>. The <code>TfidfVectorizer</code> even allows
you to specify <code>analyzer=char</code> with <code>ngram_range</code> = (3,4) to encode subwords, which even contributes to robustness against spelling errors if that's a concern. </p>
<p>The main thing that's cool about this approach is the representations can still be very reasonable for a lot of applications <em>and</em> train very quickly. Here's a quick demo:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">srsly</span>
<span class="kn">from</span> <span class="nn">umap</span> <span class="kn">import</span> <span class="n">UMAP</span>
<span class="kn">from</span> <span class="nn">cluestar</span> <span class="kn">import</span> <span class="n">plot_text</span>
<span class="kn">from</span> <span class="nn">embetter.text</span> <span class="kn">import</span> <span class="n">learn_lite_doc_embeddings</span>

<span class="c1"># Train embeddings </span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">ex</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="n">srsly</span><span class="o">.</span><span class="n">read_jsonl</span><span class="p">(</span><span class="s2">&quot;datasets/new-dataset.jsonl&quot;</span><span class="p">)]</span>
<span class="n">enc</span> <span class="o">=</span> <span class="n">learn_lite_doc_embeddings</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="c1"># Create a 2D UMAP representation</span>
<span class="n">X_orig</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>  <span class="c1"># this takes ~56ms</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">UMAP</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_orig</span><span class="p">)</span>

<span class="c1"># Plot the UMAP representation with the text</span>
<span class="n">plot_text</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">texts</span><span class="p">)</span>
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can also store the trained embeddings as part of the training-call. </p>
<div class="highlight"><pre><span></span><code><span class="n">enc</span> <span class="o">=</span> <span class="n">learn_lite_doc_embeddings</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="s2">&quot;stored/on/disk.emb&quot;</span><span class="p">)</span>
</code></pre></div>
</div>
<script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>
<script src="https://cdn.jsdelivr.net/gh/koaning/justcharts/justcharts.js"></script>

<p>Here's what this chart looks like. Note that you can click and drag to explore! </p>
<p><vegachart schema-url="../vegalite/lite_embed1.json"></vegachart></p>
<p>Let's now consider what a similar chart might look like that uses <a href="https://sbert.net">Sentence Transformers</a>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">embetter.text</span> <span class="kn">import</span> <span class="n">SentenceEncoder</span>

<span class="n">sent_enc</span> <span class="o">=</span> <span class="n">SentenceEncoder</span><span class="p">()</span>
<span class="n">X_orig</span> <span class="o">=</span> <span class="n">sent_enc</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span> <span class="c1"># this takes ~13.5s </span>
<span class="n">X</span> <span class="o">=</span> <span class="n">UMAP</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_orig</span><span class="p">)</span>
<span class="n">plot_text</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">texts</span><span class="p">)</span>
</code></pre></div>
<p><vegachart schema-url="../vegalite/lite_embed2.json"></vegachart></p>
<p>The charts differ, but if you squint you can spot a cluster on the right hand side here that 
corresponds with the cluster at the bottom of the previous chart. </p>
<p>These "litetext" embeddings do overfit on the same words being used. But they are <em>much</em> faster
and still give a reasonable representation for a lot of use-cases. </p>
<h2 id="difference-models">Difference Models</h2>
<p>Embeddings can be very useful when you're dealing with a deduplication use-case. The thinking
is that items that are close in embedded space might be great candidates to double-check. </p>
<p>To help investigate this, this library offers a <code>DifferenceModel</code> utility. </p>
<p><img alt="" src="../images/difference-model.png" /></p>
<p>Here's how you might use it. </p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">embetter.model</span> <span class="kn">import</span> <span class="n">DifferenceClassifier</span>
<span class="kn">from</span> <span class="nn">embetter.text</span> <span class="kn">import</span> <span class="n">SentenceEncoder</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">DifferenceClassifier</span><span class="p">(</span><span class="n">enc</span><span class="o">=</span><span class="n">SentenceEncoder</span><span class="p">())</span>

<span class="c1"># Suppose this is input data</span>
<span class="n">texts1</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;hello&quot;</span><span class="p">,</span> <span class="s2">&quot;firehydrant&quot;</span><span class="p">,</span> <span class="s2">&quot;greetings&quot;</span><span class="p">]</span>
<span class="n">texts2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;no&quot;</span><span class="p">,</span>    <span class="s2">&quot;yes&quot;</span><span class="p">,</span>         <span class="s2">&quot;greeting&quot;</span><span class="p">]</span>

<span class="c1"># You will need to have some definition of &quot;similar&quot;</span>
<span class="n">similar</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Train a model to detect similarity</span>
<span class="n">mod</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X1</span><span class="o">=</span><span class="n">texts1</span><span class="p">,</span> <span class="n">X2</span><span class="o">=</span><span class="n">texts2</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">similar</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X1</span><span class="o">=</span><span class="n">texts1</span><span class="p">,</span> <span class="n">X2</span><span class="o">=</span><span class="n">texts2</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X1</span><span class="o">=</span><span class="n">texts1</span><span class="p">,</span> <span class="n">X2</span><span class="o">=</span><span class="n">texts2</span><span class="p">)</span>

<span class="c1"># The classifier head is a scikit-learn model, which you could save</span>
<span class="c1"># seperately if you like. The model can be accessed via: </span>
<span class="n">mod</span><span class="o">.</span><span class="n">clf_head</span>
</code></pre></div>
<p>The model really is just a light wrapper, but it might make it easier to bootstrap.</p>
<h2 id="available-sentenceencoders">Available <code>SentenceEncoder</code>s</h2>
<p>There are <em>many</em> available models out there. Just have a look at <a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB</a>.</p>
<p>Because the <code>SentenceEncoder</code> in this library is just a wrapper around <code>sentence-transformers</code> you should also 
be able to load any more that the library can load. </p>
<div class="highlight"><pre><span></span><code><span class="c1"># https://huggingface.co/thenlper/gte-small</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceEncoder</span><span class="p">(</span><span class="s1">&#39;thenlper/gte-small&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceEncoder</span><span class="p">(</span><span class="s1">&#39;thenlper/gte-base&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceEncoder</span><span class="p">(</span><span class="s1">&#39;thenlper/gte-large&#39;</span><span class="p">)</span>
</code></pre></div>
<p>There are many more models that you can consider. Just be aware that <a href="https://huggingface.co/intfloat/e5-large-v2">some models</a> expect a prefix to be included in the text that you're encoding.</p>
<h2 id="speedup-with-modal">Speedup with Modal</h2>
<p>Embedding text can be slow, especially when you're running on a CPU. If you wish 
to speed up your embedding calculations you may enjoy using <a href="https://modal.com/">modal</a>. 
Modal allows you to add a GPU to a Python function simply by adding a decorator.</p>
<p>Not every encoder in embetter will get a speedup by using a GPU. But we've done some 
benchmarks and noticed that
<code>SentenceEncoder</code> as well as <code>ClipEncoder</code> should both benefit. These components will
also automatically detect when the GPU is available automatically.</p>
<p>The code below gives an example. </p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">h5py</span>
<span class="kn">import</span> <span class="nn">modal</span>


<span class="n">stub</span> <span class="o">=</span> <span class="n">modal</span><span class="o">.</span><span class="n">Stub</span><span class="p">(</span><span class="s2">&quot;example-get-started&quot;</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="p">(</span><span class="n">modal</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">debian_slim</span><span class="p">()</span>
         <span class="o">.</span><span class="n">pip_install</span><span class="p">(</span><span class="s2">&quot;simsity&quot;</span><span class="p">,</span> <span class="s2">&quot;embetter[text]&quot;</span><span class="p">,</span> <span class="s2">&quot;h5py&quot;</span><span class="p">)</span>
         <span class="o">.</span><span class="n">run_commands</span><span class="p">(</span><span class="s2">&quot;python -c &#39;from embetter.text import SentenceEncoder; SentenceEncoder()&#39;&quot;</span><span class="p">))</span>


<span class="c1"># This is the function that actually runs the embedding, </span>
<span class="c1"># notice that there&#39;s a GPU attached.</span>
<span class="nd">@stub</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">gpu</span><span class="o">=</span><span class="s2">&quot;any&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">create</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">embetter.text</span> <span class="kn">import</span> <span class="n">SentenceEncoder</span>
    <span class="k">return</span> <span class="n">SentenceEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>


<span class="nd">@stub</span><span class="o">.</span><span class="n">local_entrypoint</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="c1"># You&#39;d need to write your own function to read in the texts</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">read_text</span><span class="p">()</span>

    <span class="c1"># This runs our decorated function on external hardware</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">create</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># Next we save it to disk for re-use</span>
    <span class="k">with</span> <span class="n">h5py</span><span class="o">.</span><span class="n">File</span><span class="p">(</span><span class="s1">&#39;embeddings.h5&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">hf</span><span class="p">:</span>
        <span class="n">hf</span><span class="o">.</span><span class="n">create_dataset</span><span class="p">(</span><span class="s2">&quot;embeddings&quot;</span><span class="p">,</span>  <span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">)</span>
    <span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;took </span><span class="si">{</span><span class="n">toc</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">tic</span><span class="si">}</span><span class="s2">s to embed shape </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<p>On our own benchmarks, we seem to get a 4-5x speedup with just a minor edit
to the code. This can be extremely helpful when you're trying to embed data
in bulk.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["toc.integrate", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.path", "navigation.indexes", "toc.follow", "content.code.copy", "content.code.select", "content.code.annotate"], "search": "../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.220ee61c.min.js"></script>
      
    
  </body>
</html>